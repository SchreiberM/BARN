### srsh --cpus-per-task=32 --mem=48G
### snakemake -n for dryrun
### snakemake -j --cores=32 --use-conda (j for parallel jobs)
### snakemake --dag | dot -Tsvg > dag.svg
### conda env export --name ENVNAME > envname.yaml
### snakemake --cores 1 --rulegraph  | dot -Tsvg > rulegraph.svg 

import pandas as pd
import os
 
dir = "workflow/logs/"
if not os.path.exists(dir):
    os.mkdir(dir)


configfile: "config/config.yaml"

samples=pd.read_csv(config["samplesheet"], header=0).set_index(["sample"], drop=False)


rule all:
    input:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.filtered.vcf",
        "workflow/reports/multiqc_report.html"

rule fastqcF:
    input:
        lambda wildcards: samples["path"][wildcards.sample] + config["reads"][0] + config["ending"],
    output:
        html="workflow/logs/fastqc/{sample}" + config["reads"][0] + "_fastqc.html",
        zip="workflow/logs/fastqc/{sample}" + config["reads"][0]+ "_fastqc.zip"
    log:
        "workflow/logs/fastqc/{sample}" + config["reads"][0] + ".log",
    benchmark:
        "workflow/benchmark/fastqc/{sample}" + config["reads"][0] + ".txt"
    threads:
        1
    conda:
        "workflow/envs/quality.yaml"
    shell:
        "fastqc {input} -o workflow/logs/fastqc/ -t {threads} --quiet &> {log}"
    

rule fastqcR:
    input:
        lambda wildcards: samples["path"][wildcards.sample] + config["reads"][1] + config["ending"],
    output:
        html="workflow/logs/fastqc/{sample}" + config["reads"][1] + "_fastqc.html",
        zip="workflow/logs/fastqc/{sample}" + config["reads"][1]+ "_fastqc.zip"
    log:
        "workflow/logs/fastqc/{sample}" + config["reads"][1] + ".log"
    benchmark:
        "workflow/benchmark/fastqc/{sample}" + config["reads"][1] + ".txt"
    threads:
        1
    conda:
        "workflow/envs/quality.yaml"
    shell:
        "fastqc {input} -o workflow/logs/fastqc/ -t {threads} --quiet &> {log}"


rule STARIndex:
    input:
        "workflow/references/" + config["genome"]
    output:
        directory("workflow/indeces/" + config["genome"])
    log:
        "workflow/logs/indeces/star_index_genome.log"
    benchmark:
        "workflow/benchmark/star/starindex/Index.txt"
    threads:
        16
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {output} --genomeFastaFiles {input} &> {log}" 


rule STARMapping:
    input:
        r1=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][0] + config["ending"],
        r2=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][1] + config["ending"],
        index=rules.STARIndex.output
    output:
        bam=temp("workflow/results/star/firstPass/{sample}/Aligned.sortedByCoord.out.bam"),
        SpliceJunction="workflow/results/star/firstPass/{sample}/SJ.out.tab"
    log:
        "workflow/logs/star/firstPass/{sample}.log"
    benchmark:
        "workflow/benchmark/star/StarFirstPass/{sample}.txt"
    threads:
        16
    params:
        extra="--outFilterMultimapNmax 15 --alignIntronMin 60 --alignIntronMax 15000 --alignMatesGapMax 2000 --alignTranscriptsPerReadNmax 30000 --outFilterMismatchNoverLmax 0.06",
        prefix="workflow/results/star/firstPass/{sample}/"
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --genomeDir {input.index} --runThreadN {threads} --outBAMsortingThreadN {threads} {params.extra} " 
        "--readFilesIn {input.r1} {input.r2} --readFilesCommand zcat --outFileNamePrefix {params.prefix} " 
        "--outSAMprimaryFlag AllBestScore --outSAMtype BAM SortedByCoordinate --outStd Log &> {log}"


rule STARMapping2Pass:
    input:
        r1=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][0] + config["ending"],
        r2=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][1] + config["ending"],
        index=rules.STARIndex.output,
        spliceJ=expand("workflow/results/star/firstPass/{sample}/SJ.out.tab", sample=samples["sample"])
    output:
        bam=temp("workflow/results/star/secondPass/{sample}/Aligned.sortedByCoord.out.bam"),
        logFinal="workflow/results/star/secondPass/{sample}/Log.final.out"
    log:
        "workflow/logs/star/secondPass/{sample}.2pass.log"
    benchmark:
        "workflow/benchmark/star/StarSecondPass/{sample}.2pass.txt"
    threads:
        16
    params:
        extra="--outFilterMultimapNmax 15 --alignIntronMin 60 --alignIntronMax 15000 --alignMatesGapMax 2000 --alignTranscriptsPerReadNmax 30000 --outFilterMismatchNoverLmax 0.06",
        prefix="workflow/results/star/secondPass/{sample}/"
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --genomeDir {input.index} --runThreadN {threads} --outBAMsortingThreadN {threads} --sjdbFileChrStartEnd {input.spliceJ} " 
        "--readFilesIn {input.r1} {input.r2} --readFilesCommand zcat --outFileNamePrefix {params.prefix} {params.extra} " 
        "--outSAMprimaryFlag AllBestScore --outSAMtype BAM SortedByCoordinate --outStd Log &> {log}"


rule replace_rg:
    input:
        "workflow/results/star/secondPass/{sample}/Aligned.sortedByCoord.out.bam"
    output:
        temp("workflow/results/picard/fixed_rg/{sample}.bam")
    log:
        "workflow/logs/picard/replace_rg/{sample}.log"
    benchmark:
        "workflow/benchmark/picard/replace_rg/{sample}.txt"
    params:
        "RGID={sample} RGLB=lib1 RGPL=illumina RGPU={sample} RGSM={sample}"
    wrapper:
        "0.65.0/bio/picard/addorreplacereadgroups"


rule mark_duplicates:
    input:
        "workflow/results/picard/fixed_rg/{sample}.bam"
    output:
        bam="workflow/results/picard/dedup/{sample}.bam",
        metrics="workflow/results/picard/dedup/{sample}.metrics.txt"
    log:
        "workflow/logs/picard/dedup/{sample}.log"
    benchmark:
        "workflow/benchmark/picard/dedup/{sample}.txt"
    params:
        "REMOVE_DUPLICATES=false"
    wrapper:
        "0.65.0/bio/picard/markduplicates"


rule samtools_index:
    input:
        "workflow/results/picard/dedup/{sample}.bam"
    output:
        "workflow/results/picard/dedup/{sample}.bam.bai"
    benchmark:
        "workflow/benchmark/samtools/samtools.{sample}.txt"
    params:
        "" # optional params string
    wrapper:
        "0.65.0/bio/samtools/index"


rule SalmonIndex:
    input:
        transcriptome="workflow/references/" + config["transcriptome"],
        decoy="workflow/references/decoys.txt"
    output:
        dir=directory("workflow/indeces/" + config["transcriptome"])
    log:
        "workflow/logs/indeces/salmon.log"
    benchmark:
        "workflow/benchmark/salmon/salmonIndex/Index.txt"
    threads:
        16
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "salmon index -i {output.dir} -d {input.decoy} -p {threads} -t {input.transcriptome} &> {log}"


rule SalmonMapping:
    input: 
        r1=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][0] + config["ending"],
        r2=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][1] + config["ending"],
        index=rules.SalmonIndex.output
    output:
        quant="workflow/results/salmon/{sample}/quant.sf",
        log=directory("workflow/results/salmon/{sample}")
    params:
        prefix="workflow/results/salmon/{sample}"
    log:
        "workflow/logs/salmon/{sample}.log"
    benchmark:
        "workflow/benchmark/salmon/{sample}.txt"
    threads:
        8
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "salmon quant -i {input.index} -l ISR --gcBias --validateMappings -p {threads} -1 {input.r1} -2 {input.r2} -o {params.prefix} &> {log}"


rule RSeqC:
    input:
        bam="workflow/results/picard/dedup/{sample}.bam",
        bai="workflow/results/picard/dedup/{sample}.bam.bai"
    output:
        "workflow/results/rseqc/{sample}.geneBodyCoverage.txt"
    params:
        bedFile="workflow/references/" + config["transcriptCoordinates"],
        prefix="workflow/results/rseqc/{sample}"
    log:
        "workflow/logs/rseqc/{sample}"
    benchmark:
        "workflow/benchmark/rseqc/{sample}.txt"
    threads:
        1
    conda:
        "workflow/envs/rseqc.yaml"
    shell:
        """
        junction_annotation.py -i {input.bam} -r {params.bedFile} -o {params.prefix} &> {log}.junction.log
        bam_stat.py -i {input.bam} &> {log}.stats.log
        geneBody_coverage.py -i {input.bam} -r {params.bedFile} -o {params.prefix} &> {log}.geneBody.log
        """

rule Freebayes:
    input:
        samples=expand("workflow/results/picard/dedup/{sample}.bam", sample=samples["sample"]),
        bai=expand("workflow/results/picard/dedup/{sample}.bam.bai", sample=samples["sample"]),
        reference="workflow/references/" + config["genome"],
        regions="workflow/references/" + config["genomeCoordinates"]
    output:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.vcf"
    params:
        extra="--use-best-n-alleles 4 --min-base-quality 20 --min-mapping-quality 30 --min-alternate-fraction 0.8 --min-alternate-count 4 --min-coverage 8"
    benchmark:
        "workflow/benchmark/freebayes/freebayes.txt"
    threads:
        16
    conda:
        "workflow/envs/variants.yaml"
    shell:
        """
        ls {input.samples} > input.txt
        freebayes-parallel {input.regions} {threads} -f {input.reference} {params.extra} -L input.txt > {output}
        """


rule VariantFilter:
    input:
        rules.Freebayes.output
    output:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.filtered.vcf"
    params:
        extra="--minQ 30 --max-missing 0.8 --min-meanDP 8 --remove-indels"
        ## Final filtering for the big dataset should have a minor allele filter but not
        ## be used for testing
        # "--minQ 30 --max-missing 0.8 --min-meanDP 8 --maf 0.05 --remove-indels"
    benchmark:
        "workflow/benchmark/variantFilter/variantfilter.txt"
    conda:
        "workflow/envs/variants.yaml"
    shell:
        """vcftools --vcf {input} {params.extra} --recode --recode-INFO-all --stdout > {output}"""
        ## The additional SnpSift filter might be useful for the big dataset, but not for testing
        # """vcftools --vcf {input} {params.extra} --recode --stdout | SnpSift filter "(countHom()>2)" > {output}"""


rule multiQc:
    input:
        log1="workflow/logs/",
        log2=expand("workflow/results/salmon/{sample}", sample=samples["sample"]),
        log3=expand("workflow/results/star/secondPass/{sample}/Log.final.out", sample=samples["sample"]),
        log4=expand("workflow/logs/fastqc/{sample}{reads}_fastqc.zip", sample=samples["sample"], reads=config["reads"]),
        log5=expand("workflow/results/rseqc/{sample}.geneBodyCoverage.txt", sample=samples["sample"])
    output:
        "workflow/reports/multiqc_report.html"
    benchmark:
        "workflow/benchmark/mutliqc/multiqc.txt"
    conda:
        "workflow/envs/quality.yaml"
    shell:
       "multiqc {input.log1} {input.log2} {input.log3} {input.log4} {input.log5} -f --outdir workflow/reports/"