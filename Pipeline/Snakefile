### srsh --cpus-per-task=32 --mem=48G
### snakemake -n for dryrun
### snakemake -j --cores=32 --use-conda (j for parallel jobs)
### snakemake --dag | dot -Tsvg > dag.svg
### conda env export --name ENVNAME > envname.yaml

import pandas as pd

configfile: "config/config.yaml"

samples=pd.read_csv(config["samplesheet"], header=0).set_index(["sample"], drop=False)

rule all:
    input:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.filtered.vcf",
        "workflow/reports/multiqc_report.html"

rule trimming:
    input:
        r1=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][1] + config["ending"],
        r2=lambda wildcards: samples["path"][wildcards.sample] + config["reads"][2] + config["ending"]
    output:
        r1="workflow/results/trimmedReads/{sample}_1.fastq.gz",
        r2="workflow/results/trimmedReads/{sample}_2.fastq.gz",
        # reads where trimming entirely removed the mate
        r1_unpaired="workflow/results/trimmedReads/unpairedReads/{sample}_1.unpaired.fastq.gz",
        r2_unpaired="workflow/results/trimmedReads/unpairedReads/{sample}_2.unpaired.fastq.gz"
    log:
        "workflow/logs/trimmomatic/{sample}.log"
    benchmark:
        "workflow/benchmark/trimmomatic/{sample}.txt"
    params:
        # list of trimmers (see manual)
        trimmer=["ILLUMINACLIP:workflow/references/" + config["adapter"] + ":2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:60"],
        # optional parameters
        extra="",
        compression_level="-9"
    threads:
        16
    wrapper:
        "0.65.0/bio/trimmomatic/pe"


rule FastQC:
    input:
        "workflow/results/trimmedReads/{sample}_{read}.fastq.gz"
    output:
        html="workflow/logs/fastqc/{sample}_{read}_fastqc.html",
        zip="workflow/logs/fastqc/{sample}_{read}_fastqc.zip"
    log:
        "workflow/logs/fastqc/{sample}_{read}.log"
    benchmark:
        "workflow/benchmark/fastqc/{sample}_{read}.txt"
    threads:
        1
    conda:
        "workflow/envs/quality.yaml"
    shell:
        "fastqc {input} -o workflow/logs/fastqc/ -t {threads} --quiet &> {log}"
    
 
rule STARIndex:
    input:
        "workflow/references/" + config["genome"]
    output:
        directory("workflow/indeces/" + config["genome"])
    log:
        "workflow/logs/indeces/star_index_genome.log"
    benchmark:
        "workflow/benchmark/star/Index.txt"
    threads:
        16
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {output} --genomeFastaFiles {input} &> {log}" 


rule STARMapping:
    input:
        r1="workflow/results/trimmedReads/{sample}_1.fastq.gz",
        r2="workflow/results/trimmedReads/{sample}_2.fastq.gz",
        index=rules.STARIndex.output
    output:
        bam=temp("workflow/results/star/firstPass/{sample}/Aligned.sortedByCoord.out.bam"),
        SpliceJunction="workflow/results/star/firstPass/{sample}/SJ.out.tab"
    log:
        "workflow/logs/star/{sample}.log"
    benchmark:
        "workflow/benchmark/star/{sample}.txt"
    threads:
        16
    params:
        extra="--outFilterMultimapNmax 15  --alignIntronMin 60 --alignIntronMax 15000 --alignMatesGapMax 2000 --alignTranscriptsPerReadNmax 30000 --alignEndsType EndToEnd --outFilterMismatchNoverReadLmax 0.02 --outFilterMatchNminOverLread 0.98",
        prefix="workflow/results/star/firstPass/{sample}/"
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --genomeDir {input.index} --runThreadN {threads} --outBAMsortingThreadN {threads} {params.extra} " 
        "--readFilesIn {input.r1} {input.r2} --readFilesCommand zcat --outFileNamePrefix {params.prefix} " 
        "--outSAMprimaryFlag AllBestScore --outSAMtype BAM SortedByCoordinate --outStd Log &> {log}"


rule STARMapping2Pass:
    input:
        r1="workflow/results/trimmedReads/{sample}_1.fastq.gz",
        r2="workflow/results/trimmedReads/{sample}_2.fastq.gz",
        index=rules.STARIndex.output,
        spliceJ=expand("workflow/results/star/firstPass/{sample}/SJ.out.tab", sample=samples["sample"])
    output:
        bam="workflow/results/star/secondPass/{sample}/Aligned.sortedByCoord.out.bam",
        logFinal="workflow/results/star/secondPass/{sample}/Log.final.out"
    log:
        "workflow/logs/star/{sample}.2pass.log"
    benchmark:
        "workflow/benchmark/star/{sample}.2pass.txt"
    threads:
        16
    params:
        extra="--outFilterMultimapNmax 15  --alignIntronMin 60 --alignIntronMax 15000 --alignMatesGapMax 2000 --alignTranscriptsPerReadNmax 30000 --alignEndsType EndToEnd --outFilterMismatchNoverReadLmax 0.02 --outFilterMatchNminOverLread 0.98",
        prefix="workflow/results/star/secondPass/{sample}/"
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "STAR --genomeDir {input.index} --runThreadN {threads} --outBAMsortingThreadN {threads} --sjdbFileChrStartEnd {input.spliceJ} " 
        "--readFilesIn {input.r1} {input.r2} --readFilesCommand zcat --outFileNamePrefix {params.prefix} {params.extra} " 
        "--outSAMprimaryFlag AllBestScore --outSAMtype BAM SortedByCoordinate --outStd Log &> {log}"


rule replace_rg:
    input:
        "workflow/results/star/secondPass/{sample}/Aligned.sortedByCoord.out.bam"
    output:
        temp("workflow/results/picard/fixed_rg/{sample}.bam")
    log:
        "workflow/logs/picard/replace_rg/{sample}.log"
    benchmark:
        "workflow/benchmark/picard/replace_rg/{sample}.txt"
    params:
        "RGID={sample} RGLB=lib1 RGPL=illumina RGPU={sample} RGSM={sample}"
    wrapper:
        "0.65.0/bio/picard/addorreplacereadgroups"


rule mark_duplicates:
    input:
        "workflow/results/picard/fixed_rg/{sample}.bam"
    output:
        bam="workflow/results/picard/dedup/{sample}.bam",
        metrics="workflow/results/picard/dedup/{sample}.metrics.txt"
    log:
        "workflow/logs/picard/dedup/{sample}.log"
    benchmark:
        "workflow/benchmark/picard/dedup/{sample}.txt"
    params:
        "REMOVE_DUPLICATES=false"
    wrapper:
        "0.65.0/bio/picard/markduplicates"


rule samtools_index:
    input:
        "workflow/results/picard/dedup/{sample}.bam"
    output:
        "workflow/results/picard/dedup/{sample}.bam.bai"
    benchmark:
        "workflow/benchmark/picard/samtools.{sample}.txt"
    params:
        "" # optional params string
    wrapper:
        "0.65.0/bio/samtools/index"


rule SalmonIndex:
    input:
        "workflow/references/" + config["transcriptome"]
    output:
        directory("workflow/indeces/" + config["transcriptome"])
    log:
        "workflow/logs/indeces/salmon.log"
    benchmark:
        "workflow/benchmark/salmon/Index.txt"
    threads:
        16
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "salmon index -i {output} -t {input} &> {log}"
        # Still to do: check out decoys


rule SalmonMapping:
    input: 
        r1="workflow/results/trimmedReads/{sample}_1.fastq.gz",
        r2="workflow/results/trimmedReads/{sample}_2.fastq.gz",
        index=rules.SalmonIndex.output
    output:
        quant="workflow/results/salmon/{sample}/quant.sf",
        log=directory("workflow/results/salmon/{sample}")
    params:
        prefix="workflow/results/salmon/{sample}"
    log:
        "workflow/logs/salmon/{sample}.log"
    benchmark:
        "workflow/benchmark/salmon/{sample}.txt"
    threads:
        2
    conda:
        "workflow/envs/mapping.yaml"
    shell:
        "salmon quant -i {input.index} -l ISR --validateMappings -p {threads} -1 {input.r1} -2 {input.r2} -o {params.prefix} &> {log}"


rule RSeqC:
    input:
        bam="workflow/results/picard/dedup/{sample}.bam",
        bai="workflow/results/picard/dedup/{sample}.bam.bai"
    output:
        "workflow/results/rseqc/{sample}.junction.bed"
    params:
        bedFile="workflow/references/" + config["transcriptCoordinates"],
        prefix="workflow/results/rseqc/{sample}"
    log:
        "workflow/logs/rseqc/{sample}"
    benchmark:
        "workflow/benchmark/rseqc/{sample}.txt"
    threads:
        1
    conda:
        "workflow/envs/rseqc.yaml"
    shell:
        """
        junction_annotation.py -i {input.bam} -r {params.bedFile} -o {params.prefix} &> {log}.junction.log
        bam_stat.py -i {input.bam} &> {log}.stats.log
        """
        ## I found geneBody_coverage really useful, but this part of the script takes a long time
        ## so for test reasons, I've commented it out. In the final version I will include it.
        # "geneBody_coverage.py -i {input.bam} -r {params.bedFile} -o {params.prefix} &> {log}.geneBody.log"


rule Freebayes:
    input:
        samples=expand("workflow/results/picard/dedup/{sample}.bam", sample=samples["sample"]),
        bai=expand("workflow/results/picard/dedup/{sample}.bam.bai", sample=samples["sample"]),
        reference="workflow/references/" + config["genome"],
        regions="workflow/references/" + config["genomeCoordinates"]
    output:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.vcf"
    params:
        extra="--use-best-n-alleles 4 --min-base-quality 20 --min-mapping-quality 30 --min-alternate-fraction 0.8 --min-alternate-count 4 --min-coverage 8"
    benchmark:
        "workflow/benchmark/freebayes.txt"
    threads:
        16
    conda:
        "workflow/envs/variants.yaml"
    shell:
        """
        ls {input.samples} > input.txt
        freebayes-parallel {input.regions} {threads} -f {input.reference} {params.extra} -L input.txt > {output}
        """


rule VariantFilter:
    input:
        rules.Freebayes.output
    output:
        "workflow/results/freebayes/" + config["experiment"] + ".freebayes.filtered.vcf"
    params:
        extra="--minQ 30 --max-missing 0.8 --min-meanDP 8 --remove-indels"
        ## Final filtering for the big dataset should have a minor allele filter but not
        ## be used for testing
        # "--minQ 30 --max-missing 0.8 --min-meanDP 8 --maf 0.05 --remove-indels"
    benchmark:
        "workflow/benchmark/variantfilter.txt"
    conda:
        "workflow/envs/variants.yaml"
    shell:
        """vcftools --vcf {input} {params.extra} --recode --stdout > {output}"""
        ## The additional SnpSift filter might be useful for the big dataset, but not for testing
        # """vcftools --vcf {input} {params.extra} --recode --stdout | SnpSift filter "(countHom()>2)" > {output}"""


rule multiQc:
    input:
        log1="workflow/logs/",
        log2=expand("workflow/results/salmon/{sample}", sample=samples["sample"]),
        log3=expand("workflow/results/star/secondPass/{sample}/Log.final.out", sample=samples["sample"]),
        log4=expand("workflow/logs/fastqc/{sample}_{read}_fastqc.zip", sample=samples["sample"], read=["1", "2"]),
        log5=expand("workflow/results/rseqc/{sample}.junction.bed", sample=samples["sample"])
    output:
        "workflow/reports/multiqc_report.html"
    benchmark:
        "workflow/benchmark/multiqc.txt"
    conda:
        "workflow/envs/quality.yaml"
    shell:
       "multiqc {input.log1} {input.log2} {input.log3} {input.log4} -f --outdir workflow/reports/"